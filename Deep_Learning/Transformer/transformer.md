# Transformer
paper: https://arxiv.org/abs/1706.03762 

## Basic concepts 

### Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.

## Q&A

1. Transformer 为何使用 Multi-Head Attention 机制？
